# 线性分类、损失函数与梯度下降

<vision.stanford.edu/teaching/cs231n-demos/linear-classify/>

### 损失函数

铰链损失函数
![](img2/1.png)

错误值与正确值的差加1与0的最大值再求和

最小值为0，最大值为正无穷
![](img2/2.png)

正则化

![](img2/3.png)

正则化避免过拟合
让权重更加简单，在测试集上更好的泛化

![](img2/4.png)

### softmax分类器 多分类的逻辑回归

将识别的分数转化为概率，先将分数使用指数函数  $e^x$ 特殊处理，可以将负数转化为正数,然后进行归一化

![](img2/5.png)

### 交叉熵损    失函数

![](img2/6.png)

最小值为0，最大值为正无穷

![](img2/7.png)

数值解：慢
解析解：快

### 梯度

![](img2/8.png)
数值解：速度慢

解析解：使用求导公式，速度快

梯度下降的下降是使得损失函数下降，而不是梯度本身下降
![](img2/9.png)


### 课后作业解析

完全向量化，不使用for循环计算L2距离

![](img2/11.png)

```lan=py
# keepdims = true 是指维持原来的维度

import numpy as np
a = np.array([[1,2],[3,4]])

# 按行相加，并且保持其二维特性
print(np.sum(a, axis=1, keepdims=True))

# 按行相加，不保持其二维特性
print(np.sum(a, axis=1))

array([[3], [7]])
array([3, 7])


# np.array_split(arr, len, axis)
# axis为划分的维度
# 不写默认为0，说当前维度为1，则转化到下一维度
```


### softmax 梯度求解
![](img2/12%20(1).png)
![](img2/12%20(2).png)
![](img2/12%20(3).png)